{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inventor-singh/langchain/blob/master/examples/document_workflows/contract_review/contract_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65d3850-3ddb-4db3-87fe-fa7d92e6c55b",
      "metadata": {
        "id": "a65d3850-3ddb-4db3-87fe-fa7d92e6c55b"
      },
      "source": [
        "# Contract Review Workflow\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/contract_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "![](https://github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/contract_review.png?raw=1)\n",
        "\n",
        "This tutorial shows you how to create an agentic workflow that can review a contract for compliance with certain regulations. We will parse the contract into a set of key clauses, match it with relevant clauses from a guideline repository (here, we specifically do GDPR), and then produce a compliance summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "87c5f0d7-c884-475a-88ff-4b7057bb91a6",
      "metadata": {
        "collapsed": true,
        "id": "87c5f0d7-c884-475a-88ff-4b7057bb91a6",
        "outputId": "517d9589-6a64-4511-83d0-c3bfadd272ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.33-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-cloud\n",
            "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.6.16-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.33 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.33.post1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud) (2025.1.31)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.11/dist-packages (from llama-cloud) (2.11.3)\n",
            "Collecting llama-cloud-services>=0.6.16 (from llama-parse)\n",
            "  Downloading llama_cloud_services-0.6.16-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud) (0.14.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.16->llama-parse) (8.1.8)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.16->llama-parse) (4.3.7)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.16->llama-parse)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.75.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (3.11.15)\n",
            "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (9.1.2)\n",
            "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (4.13.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.17.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10->llama-cloud) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10->llama-cloud) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10->llama-cloud) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.20.0)\n",
            "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.2.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.33->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.33-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.16-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.16-py3-none-any.whl (36 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.33.post1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, colorama, typing-inspect, tiktoken, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed banks-2.1.2 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.19 llama-cloud-services-0.6.16 llama-index-0.12.33 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.33.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.38 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.16 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-5.4.0 python-dotenv-1.1.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-indices-managed-llama-cloud llama-cloud llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8fe98ed0-cfcb-4c37-ac5a-7140d79fefd0",
      "metadata": {
        "id": "8fe98ed0-cfcb-4c37-ac5a-7140d79fefd0"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d76541e7-65d3-4c70-afcd-49658bc00954",
      "metadata": {
        "id": "d76541e7-65d3-4c70-afcd-49658bc00954"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We setup an index for guidelines. In this case it's just the GDPR document.\n",
        "\n",
        "We also setup our parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dec74e08-72fc-4ed8-9e13-e1ed609ed890",
      "metadata": {
        "id": "dec74e08-72fc-4ed8-9e13-e1ed609ed890",
        "outputId": "e3225100-0cdf-456d-ead4-dfd11141abe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-26 20:11:51--  https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679\n",
            "Resolving eur-lex.europa.eu (eur-lex.europa.eu)... 99.84.252.25, 99.84.252.46, 99.84.252.86, ...\n",
            "Connecting to eur-lex.europa.eu (eur-lex.europa.eu)|99.84.252.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: ‘data/gdpr.pdf’\n",
            "\n",
            "data/gdpr.pdf           [  <=>               ] 959.27K  2.74MB/s    in 0.3s    \n",
            "\n",
            "2025-04-26 20:11:52 (2.74 MB/s) - ‘data/gdpr.pdf’ saved [982296]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data\n",
        "!wget \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679\" -O data/gdpr.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01c7a8f-25f9-4f3b-b040-ee15d34826f6",
      "metadata": {
        "id": "e01c7a8f-25f9-4f3b-b040-ee15d34826f6"
      },
      "source": [
        "### Setup Index\n",
        "Here we use LlamaCloud: https://cloud.llamaindex.ai/. If you don't have access yet, you're always welcome to use our open-source VectorStoreIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ccec03-9aaf-42c7-99e1-f7187328548b",
      "metadata": {
        "id": "07ccec03-9aaf-42c7-99e1-f7187328548b"
      },
      "outputs": [],
      "source": [
        "# option 1\n",
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "  name=\"gdpr\",\n",
        "  project_name=\"Default\",\n",
        "  organization_id=\"74145918-ca39-4837-801a-c5adfc843b7c\",\n",
        "  api_key=\"llx-JrURyPDAXgOvjxaESrTI3eRHQp4OBvVf7eKfDWPjqkcQd0i8\"\n",
        ")\n",
        "\n",
        "retriever = index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5d1dfc-5d1e-4acd-80c9-616bd9aff3cb",
      "metadata": {
        "id": "1e5d1dfc-5d1e-4acd-80c9-616bd9aff3cb"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb66d70-58e2-46cf-8dc7-9188a70891f3",
      "metadata": {
        "id": "2bb66d70-58e2-46cf-8dc7-9188a70891f3"
      },
      "source": [
        "### Setup Parser\n",
        "\n",
        "Here we use LlamaParse to parse the vendor agremeent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af32038-7ce0-49f0-9d67-bf7260a3f27f",
      "metadata": {
        "id": "8af32038-7ce0-49f0-9d67-bf7260a3f27f"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "# use our multimodal models for extractions\n",
        "parser = LlamaParse(result_type=\"markdown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e634cc0d-fc3f-4864-ae64-7d681647cc10",
      "metadata": {
        "id": "e634cc0d-fc3f-4864-ae64-7d681647cc10"
      },
      "source": [
        "### Define Contract Output Schema\n",
        "\n",
        "We want to extract relevant clauses from the agreement in order to match it against relevant clauses in the GDPR. This schema defines a way to structuring the set of extracted clauses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3aceab-26cb-424d-9b24-624e11550901",
      "metadata": {
        "id": "8d3aceab-26cb-424d-9b24-624e11550901"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class ContractClause(BaseModel):\n",
        "    clause_text: str = Field(..., description=\"The exact text of the clause.\")\n",
        "    mentions_data_processing: bool = Field(False, description=\"True if the clause involves personal data collection or usage.\")\n",
        "    mentions_data_transfer: bool = Field(False, description=\"True if the clause involves transferring personal data, especially to third parties or across borders.\")\n",
        "    requires_consent: bool = Field(False, description=\"True if the clause explicitly states that user consent is needed for data activities.\")\n",
        "    specifies_purpose: bool = Field(False, description=\"True if the clause specifies a clear purpose for data handling or transfer.\")\n",
        "    mentions_safeguards: bool = Field(False, description=\"True if the clause mentions security measures or other safeguards for data.\")\n",
        "\n",
        "class ContractExtraction(BaseModel):\n",
        "    vendor_name: Optional[str] = Field(None, description=\"The vendor's name if identifiable.\")\n",
        "    effective_date: Optional[str] = Field(None, description=\"The effective date of the agreement, if available.\")\n",
        "    governing_law: Optional[str] = Field(None, description=\"The governing law of the contract, if stated.\")\n",
        "    clauses: List[ContractClause] = Field(..., description=\"List of extracted clauses and their relevant indicators.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4a1d37-c8c8-48ab-b394-51bb2412f3b5",
      "metadata": {
        "id": "9a4a1d37-c8c8-48ab-b394-51bb2412f3b5"
      },
      "source": [
        "### Define Compliance Check Schema\n",
        "\n",
        "Define a schema that matches clauses with relevant guidelines in GDPR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0e0f4f-5f82-46b0-92a2-3e47fc061de3",
      "metadata": {
        "id": "7d0e0f4f-5f82-46b0-92a2-3e47fc061de3"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GuidelineMatch(BaseModel):\n",
        "    guideline_text: str = Field(..., description=\"The single most relevant guideline excerpt related to this clause.\")\n",
        "    similarity_score: float = Field(..., description=\"Similarity score indicating how closely the guideline matches the clause, e.g., between 0 and 1.\")\n",
        "    relevance_explanation: Optional[str] = Field(None, description=\"Brief explanation of why this guideline is relevant.\")\n",
        "\n",
        "class ClauseComplianceCheck(BaseModel):\n",
        "    clause_text: str = Field(..., description=\"The exact text of the clause from the contract.\")\n",
        "    matched_guideline: Optional[GuidelineMatch] = Field(None, description=\"The most relevant guideline extracted via vector retrieval.\")\n",
        "    compliant: bool = Field(..., description=\"Indicates whether the clause is considered compliant with the referenced guideline.\")\n",
        "    notes: Optional[str] = Field(None, description=\"Additional commentary or recommendations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e1805c-a9ad-433c-8f22-a005de3fe0ab",
      "metadata": {
        "id": "15e1805c-a9ad-433c-8f22-a005de3fe0ab"
      },
      "source": [
        "### Define Final Output Schema\n",
        "\n",
        "This is the schema for the final compliance report. It contains the vendor name, if it's overall compliant, and also the summary notes.\n",
        "\n",
        "It will be inferred from the individual checks for every clause."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f275f4-66cc-4950-93f7-47da98d14e96",
      "metadata": {
        "id": "b9f275f4-66cc-4950-93f7-47da98d14e96"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class ComplianceReport(BaseModel):\n",
        "    vendor_name: Optional[str] = Field(None, description=\"The vendor's name if identified from the contract.\")\n",
        "    overall_compliant: bool = Field(..., description=\"Indicates if the contract is considered overall compliant.\")\n",
        "    summary_notes: Optional[str] = Field(None, description=\"General summary or recommendations for achieving full compliance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff8eebc-c00e-4499-ad5d-2d7843786eba",
      "metadata": {
        "id": "0ff8eebc-c00e-4499-ad5d-2d7843786eba"
      },
      "source": [
        "## Setup Contract Review Workflow\n",
        "\n",
        "Let's define the following contract review workflow:\n",
        "1. Extract out structured data from the vendor agreement.\n",
        "2. For each clause, do retrieval against GDPR to see if it's compliant with guidelines.\n",
        "3. Generate a final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b803856-630c-497c-9142-bd5aeb9efa5d",
      "metadata": {
        "id": "8b803856-630c-497c-9142-bd5aeb9efa5d"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import (\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Context,\n",
        "    Workflow,\n",
        "    step,\n",
        ")\n",
        "from llama_index.core.llms import LLM\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.prompts import ChatPromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "_logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "CONTRACT_EXTRACT_PROMPT = \"\"\"\\\n",
        "You are given contract data below. \\\n",
        "Please extract out relevant information from the contract into the defined schema - the schema is defined as a function call.\\\n",
        "\n",
        "{contract_data}\n",
        "\"\"\"\n",
        "\n",
        "CONTRACT_MATCH_PROMPT = \"\"\"\\\n",
        "Given the following contract clause and the corresponding relevant guideline text, evaluate the compliance \\\n",
        "and provide a JSON object that matches the ClauseComplianceCheck schema.\n",
        "\n",
        "**Contract Clause:**\n",
        "{clause_text}\n",
        "\n",
        "**Matched Guideline Text(s):**\n",
        "{guideline_text}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "COMPLIANCE_REPORT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a compliance reporting assistant. Your task is to generate a final compliance report \\\n",
        "based on the results of clause compliance checks against \\\n",
        "a given set of guidelines.\n",
        "\n",
        "Analyze the provided compliance results and produce a structured report according to the specified schema.\n",
        "Ensure that if there are no noncompliant clauses, the report clearly indicates full compliance.\n",
        "\"\"\"\n",
        "\n",
        "COMPLIANCE_REPORT_USER_PROMPT = \"\"\"\\\n",
        "A set of clauses within a contract were checked against GDPR compliance guidelines for the following vendor: {vendor_name}.\n",
        "The set of noncompliant clauses are given below.\n",
        "\n",
        "Each section includes:\n",
        "- **Clause:** The exact text of the contract clause.\n",
        "- **Guideline:** The relevant GDPR guideline text.\n",
        "- **Compliance Status:** Should be `False` for noncompliant clauses.\n",
        "- **Notes:** Additional information or explanations.\n",
        "\n",
        "{compliance_results}\n",
        "\n",
        "Based on the above compliance results, generate a final compliance report following the `ComplianceReport` schema below.\n",
        "If there are no noncompliant clauses, the report should indicate that the contract is fully compliant.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ContractExtractionEvent(Event):\n",
        "    contract_extraction: ContractExtraction\n",
        "\n",
        "\n",
        "class MatchGuidelineEvent(Event):\n",
        "    clause: ContractClause\n",
        "\n",
        "\n",
        "class MatchGuidelineResultEvent(Event):\n",
        "    result: ClauseComplianceCheck\n",
        "\n",
        "\n",
        "class GenerateReportEvent(Event):\n",
        "    match_results: List[ClauseComplianceCheck]\n",
        "\n",
        "\n",
        "class LogEvent(Event):\n",
        "    msg: str\n",
        "    delta: bool = False\n",
        "\n",
        "\n",
        "class ContractReviewWorkflow(Workflow):\n",
        "    \"\"\"Contract review workflow.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        parser: LlamaParse,\n",
        "        guideline_retriever: BaseRetriever,\n",
        "        llm: LLM | None = None,\n",
        "        similarity_top_k: int = 20,\n",
        "        output_dir: str = \"data_out\",\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.parser = parser\n",
        "        self.guideline_retriever = guideline_retriever\n",
        "\n",
        "        self.llm = llm or OpenAI(model=\"gpt-4o-mini\")\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "\n",
        "        # if not exists, create\n",
        "        out_path = Path(output_dir) / \"workflow_output\"\n",
        "        if not out_path.exists():\n",
        "            out_path.mkdir(parents=True, exist_ok=True)\n",
        "            os.chmod(str(out_path), 0o0777)\n",
        "        self.output_dir = out_path\n",
        "\n",
        "    @step\n",
        "    async def parse_contract(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> ContractExtractionEvent:\n",
        "        # load output template file\n",
        "        contract_extraction_path = Path(\n",
        "            f\"{self.output_dir}/contract_extraction.json\"\n",
        "        )\n",
        "        if contract_extraction_path.exists():\n",
        "            if self._verbose:\n",
        "                ctx.write_event_to_stream(LogEvent(msg=\">> Loading contract from cache\"))\n",
        "            contract_extraction_dict = json.load(open(str(contract_extraction_path), \"r\"))\n",
        "            contract_extraction = ContractExtraction.model_validate(contract_extraction_dict)\n",
        "        else:\n",
        "            if self._verbose:\n",
        "                ctx.write_event_to_stream(LogEvent(msg=\">> Reading contract\"))\n",
        "\n",
        "            # no need to parse contract, it's already in markdown\n",
        "            # you can use LlamaParse to parse more complex PDFs + other docs\n",
        "\n",
        "            docs = SimpleDirectoryReader(input_files=[ev.contract_path]).load_data()\n",
        "\n",
        "            # extract from contract\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"user\", CONTRACT_EXTRACT_PROMPT)\n",
        "            ])\n",
        "            contract_extraction = await llm.astructured_predict(\n",
        "                ContractExtraction,\n",
        "                prompt,\n",
        "                contract_data=\"\\n\".join([d.get_content(metadata_mode=\"all\") for d in docs])\n",
        "            )\n",
        "            if not isinstance(contract_extraction, ContractExtraction):\n",
        "                raise ValueError(f\"Invalid extraction from contract: {contract_extraction}\")\n",
        "            # save output template to file\n",
        "            with open(contract_extraction_path, \"w\") as fp:\n",
        "                fp.write(contract_extraction.model_dump_json())\n",
        "        if self._verbose:\n",
        "            ctx.write_event_to_stream(LogEvent(msg=f\">> Contract data: {contract_extraction.dict()}\"))\n",
        "\n",
        "        return ContractExtractionEvent(contract_extraction=contract_extraction)\n",
        "\n",
        "    @step\n",
        "    async def dispatch_guideline_match(\n",
        "        self, ctx: Context, ev: ContractExtractionEvent\n",
        "    ) -> MatchGuidelineEvent:\n",
        "        \"\"\"For each clause in the contract, find relevant guidelines.\n",
        "\n",
        "        Use a map-reduce pattern.\n",
        "\n",
        "        \"\"\"\n",
        "        await ctx.set(\"num_clauses\", len(ev.contract_extraction.clauses))\n",
        "        await ctx.set(\"vendor_name\", ev.contract_extraction.vendor_name)\n",
        "\n",
        "        for clause in ev.contract_extraction.clauses:\n",
        "            ctx.send_event(MatchGuidelineEvent(clause=clause, vendor_name=ev.contract_extraction.vendor_name))\n",
        "\n",
        "    @step\n",
        "    async def handle_guideline_match(\n",
        "        self, ctx: Context, ev: MatchGuidelineEvent\n",
        "    ) -> MatchGuidelineResultEvent:\n",
        "        \"\"\"Handle matching clause against guideline.\"\"\"\n",
        "\n",
        "        # retrieve matching guideline\n",
        "        query = f\"\"\"\\\n",
        "Please find the relevant guideline from {ev.vendor_name} that aligns with the following contract clause:\n",
        "\n",
        "{ev.clause.clause_text}\n",
        "\"\"\"\n",
        "        guideline_docs = self.guideline_retriever.retrieve(query)\n",
        "        guideline_text=\"\\n\\n\".join([g.get_content() for g in guideline_docs])\n",
        "        if self._verbose:\n",
        "            ctx.write_event_to_stream(\n",
        "                LogEvent(msg=f\">> Found guidelines: {guideline_text[:200]}...\")\n",
        "            )\n",
        "\n",
        "        # extract from contract\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"user\", CONTRACT_MATCH_PROMPT)\n",
        "        ])\n",
        "        compliance_output = await llm.astructured_predict(\n",
        "            ClauseComplianceCheck,\n",
        "            prompt,\n",
        "            clause_text=ev.clause.model_dump_json(),\n",
        "            guideline_text=guideline_text\n",
        "\n",
        "        )\n",
        "\n",
        "        if not isinstance(compliance_output, ClauseComplianceCheck):\n",
        "            raise ValueError(f\"Invalid compliance check: {compliance_output}\")\n",
        "\n",
        "        return MatchGuidelineResultEvent(result=compliance_output)\n",
        "\n",
        "    @step\n",
        "    async def gather_guideline_match(\n",
        "        self, ctx: Context, ev: MatchGuidelineResultEvent\n",
        "    ) -> GenerateReportEvent:\n",
        "        \"\"\"Handle matching clause against guideline.\"\"\"\n",
        "        num_clauses = await ctx.get(\"num_clauses\")\n",
        "        events = ctx.collect_events(ev, [MatchGuidelineResultEvent] * num_clauses)\n",
        "        if events is None:\n",
        "            return\n",
        "\n",
        "        match_results = [e.result for e in events]\n",
        "        # save match results\n",
        "        match_results_path = Path(\n",
        "            f\"{self.output_dir}/match_results.jsonl\"\n",
        "        )\n",
        "        with open(match_results_path, \"w\") as fp:\n",
        "            for mr in match_results:\n",
        "                fp.write(mr.model_dump_json() + \"\\n\")\n",
        "\n",
        "\n",
        "        return GenerateReportEvent(match_results=[e.result for e in events])\n",
        "\n",
        "    @step\n",
        "    async def generate_output(\n",
        "        self, ctx: Context, ev: GenerateReportEvent\n",
        "    ) -> StopEvent:\n",
        "        if self._verbose:\n",
        "            ctx.write_event_to_stream(LogEvent(msg=\">> Generating Compliance Report\"))\n",
        "\n",
        "        # if all clauses are compliant, return a compliant result\n",
        "        non_compliant_results = [r for r in ev.match_results if not r.compliant]\n",
        "\n",
        "        # generate compliance results string\n",
        "        result_tmpl = \"\"\"\n",
        "1. **Clause**: {clause}\n",
        "2. **Guideline:** {guideline}\n",
        "3. **Compliance Status:** {compliance_status}\n",
        "4. **Notes:** {notes}\n",
        "\"\"\"\n",
        "        non_compliant_strings = []\n",
        "        for nr in non_compliant_results:\n",
        "            non_compliant_strings.append(\n",
        "                result_tmpl.format(\n",
        "                    clause=nr.clause_text,\n",
        "                    guideline=nr.matched_guideline.guideline_text,\n",
        "                    compliance_status=nr.compliant,\n",
        "                    notes=nr.notes\n",
        "                )\n",
        "            )\n",
        "        non_compliant_str = \"\\n\\n\".join(non_compliant_strings)\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", COMPLIANCE_REPORT_SYSTEM_PROMPT),\n",
        "            (\"user\", COMPLIANCE_REPORT_USER_PROMPT)\n",
        "        ])\n",
        "        compliance_report = await llm.astructured_predict(\n",
        "            ComplianceReport,\n",
        "            prompt,\n",
        "            compliance_results=non_compliant_str,\n",
        "            vendor_name=await ctx.get(\"vendor_name\")\n",
        "        )\n",
        "\n",
        "        return StopEvent(result={\"report\": compliance_report, \"non_compliant_results\": non_compliant_results})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8557f4a7-ce61-4757-8f35-6785574c57cb",
      "metadata": {
        "id": "8557f4a7-ce61-4757-8f35-6785574c57cb"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "workflow = ContractReviewWorkflow(\n",
        "    parser=parser,\n",
        "    guideline_retriever=retriever,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    timeout=None,  # don't worry about timeout to make sure it completes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d79ab2-1455-4f8f-bde3-8bbd51b7da51",
      "metadata": {
        "id": "66d79ab2-1455-4f8f-bde3-8bbd51b7da51"
      },
      "source": [
        "#### Visualize the workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470f298b-c3a3-485e-a440-b0576e45134d",
      "metadata": {
        "id": "470f298b-c3a3-485e-a440-b0576e45134d"
      },
      "outputs": [],
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(ContractReviewWorkflow, filename=\"contract_workflow.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f11484c-52b8-424f-9a64-7456068ff1b1",
      "metadata": {
        "id": "0f11484c-52b8-424f-9a64-7456068ff1b1"
      },
      "source": [
        "## Run the Workflow\n",
        "\n",
        "Let's run the full workflow and generate the output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a55b1b-8263-4365-b187-6204150ec4cb",
      "metadata": {
        "scrolled": true,
        "id": "b9a55b1b-8263-4365-b187-6204150ec4cb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "handler = workflow.run(contract_path=\"data/vendor_agreement.md\")\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, LogEvent):\n",
        "        if event.delta:\n",
        "            print(event.msg, end=\"\")\n",
        "        else:\n",
        "            print(event.msg)\n",
        "\n",
        "response_dict = await handler\n",
        "print(str(response_dict[\"report\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0f5f52-ced9-49c2-82b9-131470fd291e",
      "metadata": {
        "id": "bb0f5f52-ced9-49c2-82b9-131470fd291e",
        "outputId": "b0918170-e48c-48cb-a277-1657637f5492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vendor_name='ACME Office Supply, Inc.' overall_compliant=False summary_notes=\"The contract contains noncompliant clauses regarding subprocessors and data transfer. It allows engaging subprocessors without prior client approval and lacks the client's right to object. Additionally, it does not mention additional safeguards or compliance with standard contractual clauses for data transfer, which are recommended to protect data subjects' rights. To achieve full compliance, these clauses should be revised to align with GDPR guidelines.\"\n"
          ]
        }
      ],
      "source": [
        "print(str(response_dict[\"report\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda1efaa-f7cf-4a05-a2f7-13bcb15d26ef",
      "metadata": {
        "id": "dda1efaa-f7cf-4a05-a2f7-13bcb15d26ef",
        "outputId": "5328d68d-cdb5-4d9b-a6ec-613ba4a04137"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ClauseComplianceCheck(clause_text='- Vendor may engage subprocessors without prior Client approval - Subprocessors may be located in any jurisdiction globally - Notice of new subprocessors provided within 30 days of engagement - Client has no right to object to new subprocessors', matched_guideline=GuidelineMatch(guideline_text='The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.', similarity_score=0.9, relevance_explanation='The guideline specifies that the processor must obtain prior authorization from the controller before engaging subprocessors, and must inform the controller of changes, allowing them to object. The contract clause does not comply with these requirements.'), compliant=False, notes='The contract clause does not comply with the guideline as it allows the vendor to engage subprocessors without prior client approval and does not provide the client the right to object to new subprocessors.'),\n",
              " ClauseComplianceCheck(clause_text='- Vendor maintains primary data centers in the United States - Vendor may transfer data to any country where it maintains operations - No prior notification required for new data storage locations - Vendor will rely on its standard data transfer mechanisms - Data may be processed by staff operating outside the EEA', matched_guideline=GuidelineMatch(guideline_text='Standard data-protection clauses in a wider contract, such as a contract between the processor and another processor, nor from adding other clauses or additional safeguards provided that they do not contradict, directly or indirectly, the standard contractual clauses adopted by the Commission or by a supervisory authority or prejudice the fundamental rights or freedoms of the data subjects. Controllers and processors should be encouraged to provide additional safeguards via contractual commitments that supplement standard protection clauses.', similarity_score=0.85, relevance_explanation=\"The guideline emphasizes the importance of standard data-protection clauses and additional safeguards, which is relevant to the clause's mention of standard data transfer mechanisms.\"), compliant=False, notes=\"The clause lacks mention of additional safeguards or compliance with standard contractual clauses, which are recommended by the guideline to ensure protection of data subjects' rights.\")]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_dict[\"non_compliant_results\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710ae172-b7e7-4d95-960f-f9cd90b40e3f",
      "metadata": {
        "id": "710ae172-b7e7-4d95-960f-f9cd90b40e3f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamacloud-demo",
      "language": "python",
      "name": "llamacloud-demo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}